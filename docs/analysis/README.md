# FSDP2 åŽç«¯å®žçŽ°å®Œæ•´åˆ†æžæ–‡æ¡£ç´¢å¼•

## æ–‡æ¡£æ¦‚è¿°

æœ¬ç›®å½•åŒ…å«å¯¹ slime æ¡†æž¶ä¸­ FSDP2ï¼ˆFully Sharded Data Parallel v2ï¼‰åŽç«¯çš„å®Œæ•´æºç çº§åˆ†æžã€‚è¿™äº›æ–‡æ¡£ä¸“ä¸ºå¸Œæœ›æ·±å…¥ç†è§£ FSDP2 å®žçŽ°ç»†èŠ‚å¹¶å°†å…¶å¤åˆ¶åˆ°å…¶ä»–æ¡†æž¶çš„å¼€å‘è€…è®¾è®¡ã€‚

**åˆ†æžåŽŸåˆ™**ï¼š
- âœ… æ‰€æœ‰åˆ†æžåŸºäºŽæ¡†æž¶æºç ï¼Œä¸å‡­ç©ºæé€ 
- âœ… åŒ…å«å…·ä½“ä»£ç è¡Œå·å’Œå®žçŽ°ç»†èŠ‚
- âœ… æä¾›æ€§èƒ½è®¡ç®—å’Œé€šä¿¡å¼€é”€åˆ†æž
- âœ… å…³æ³¨æ•°æ®æµã€å†…å­˜ç®¡ç†ã€å¹¶è¡Œé€šä¿¡ã€ç”Ÿæ€å…¼å®¹

---

## ðŸ“š æ–‡æ¡£ç»“æž„ä¸Žå­¦ä¹ è·¯å¾„

### ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒåŸºç¡€æž¶æž„ (Problems 1-4)

#### 1. [FSDP2 åŸºç¡€å®žçŽ°æ·±åº¦å‰–æž](fsdp2_implementation_deep_dive.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šFSDP2 çš„åŸºæœ¬å®žçŽ°æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- FSDP2 ä¸Ž FSDP1 çš„æ ¸å¿ƒå·®å¼‚ï¼ˆDTensorã€å…¨æ–° APIï¼‰
- `apply_fsdp2()` å®žçŽ°ç»†èŠ‚ï¼ˆactor.py:1016-1057ï¼‰
- æ¨¡å—åŒ…è£¹ç­–ç•¥ï¼ˆDecoder Layerã€Embeddingã€LM Headï¼‰
- CPUOffloadPolicy ä¸Ž MixedPrecisionPolicy

**é€‚åˆäººç¾¤**ï¼šé¦–æ¬¡æŽ¥è§¦ FSDP2ï¼Œéœ€è¦æ•´ä½“ç†è§£å…¶å·¥ä½œåŽŸç†

**å…³é”®å‘çŽ°**ï¼š
- FSDP2 åŸºäºŽ DTensor å®žçŽ°ï¼Œæ”¯æŒ CPU offload + BF16/FP32 æ··åˆç²¾åº¦
- é‡‡ç”¨ layer-wise shardingï¼Œæ¯ä¸ª Decoder Layer ç‹¬ç«‹åŒ…è£¹
- Embedding å±‚ç‰¹æ®Šå¤„ç†ï¼ˆtie_word_embeddings åˆ¤æ–­ï¼‰

---

#### 2. [DeviceMesh ä¸Žåˆ†ç‰‡æœºåˆ¶æ·±åº¦å‰–æž](fsdp2_devicemesh_and_sharding_deep_dive.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šFSDP2 å¦‚ä½•ç»„ç»‡ GPU æ‹“æ‰‘å¹¶è¿›è¡Œå‚æ•°åˆ†ç‰‡ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- 2D DeviceMesh æž„å»ºï¼š`(dp_size, cp_size)` ç½‘æ ¼ï¼ˆactor.py:165-209ï¼‰
- `dp_group` vs `cp_group` ç”¨é€”åŒºåˆ†
- FSDP åœ¨ `dp_mesh` ä¸Šçš„æ¢¯åº¦åŒæ­¥æœºåˆ¶
- Ring Flash Attention åœ¨ `cp_group` ä¸Šçš„ KV ä¼ é€’

**é€‚åˆäººç¾¤**ï¼šéœ€è¦ç†è§£å¤šç»´å¹¶è¡Œç­–ç•¥å’Œé€šä¿¡æ‹“æ‰‘

**å…³é”®å‘çŽ°**ï¼š
- 2D Mesh å®žçŽ° DP + CP æ··åˆå¹¶è¡Œ
- DP ç»´åº¦ç”¨äºŽ FSDP å‚æ•°åˆ†ç‰‡å’Œæ¢¯åº¦è§„çº¦
- CP ç»´åº¦ç”¨äºŽåºåˆ—åˆ‡åˆ†å’Œ Ring Attention

**é€šä¿¡å¼€é”€**ï¼ˆ7B æ¨¡åž‹ï¼Œseq_len=4096ï¼Œdp=4ï¼Œcp=2ï¼‰ï¼š
- FSDP æ¢¯åº¦ all-reduceï¼š25.2 GB/stepï¼ˆè·¨ dp_groupï¼‰
- Ring Attention KV ä¼ é€’ï¼š128 MB/layerï¼ˆè·¨ cp_groupï¼‰

---

#### 3. [Checkpoint ä¸Ž HuggingFace å…¼å®¹æ€§åˆ†æž](fsdp2_checkpoint_and_huggingface_compatibility.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šFSDP2 å¦‚ä½•å®žçŽ°åˆ†å¸ƒå¼ checkpoint å¹¶ä¸Ž HuggingFace äº’è½¬ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- `torch.distributed.checkpoint` åˆ†ç‰‡ä¿å­˜ï¼ˆcheckpoint.py:93-146ï¼‰
- `_fsdp2_load_full_state_dict()` å®Œæ•´åŠ è½½ï¼ˆactor.py:1059-1088ï¼‰
- è½¬æ¢å·¥å…·ï¼š`convert_hf_to_torch_dist.py` ä¸Ž `convert_torch_dist_to_hf.py`

**é€‚åˆäººç¾¤**ï¼šéœ€è¦å®žçŽ° checkpoint ç³»ç»Ÿæˆ–ä¸Ž HuggingFace ç”Ÿæ€é›†æˆ

**å…³é”®å‘çŽ°**ï¼š
- åˆ†ç‰‡ä¿å­˜é¿å… OOMï¼ˆæ¯ä¸ª rank åªä¿å­˜ 1/N å‚æ•°ï¼‰
- åŠ è½½æ—¶æ”¯æŒ CPU offloadï¼ˆé¿å…ä¸´æ—¶æ˜¾å­˜å³°å€¼ï¼‰
- éœ€è¦æ‰‹åŠ¨å®žçŽ°ä¸Ž HuggingFace æ ¼å¼äº’è½¬

---

#### 4. [åˆ†ç‰‡ç²’åº¦ä¸Ž Eager Mode åˆ†æž](fsdp2_sharding_granularity_analysis.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šFSDP2 çš„åˆ†ç‰‡ç²’åº¦å¦‚ä½•æŽ§åˆ¶ï¼Ÿä¸ºä½•ä¸æ”¯æŒ Compile Modeï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- `layer_cls_to_wrap` é…ç½®ï¼ˆactor.py:1016-1057ï¼‰
- è‡ªåŠ¨å­æ¨¡å—åŒ…è£¹ï¼ˆAttentionã€MLPã€Embeddingï¼‰
- Eager Mode é™åˆ¶åŽŸå› ï¼ˆDTensorã€åŠ¨æ€æ•°æ®åŒ…ï¼‰

**é€‚åˆäººç¾¤**ï¼šéœ€è¦ä¼˜åŒ–åˆ†ç‰‡ç­–ç•¥æˆ–ç†è§£ç¼–è¯‘é™åˆ¶

**å…³é”®å‘çŽ°**ï¼š
- åˆ†ç‰‡ç²’åº¦ = Layer-wiseï¼ˆæ¯ä¸ª Decoder Layer ç‹¬ç«‹ï¼‰
- å­æ¨¡å—è‡ªåŠ¨åŒ…è£¹ï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼‰
- DTensor å¯¼è‡´æ— æ³•ä½¿ç”¨ torch.compile

---

#### 5. [Mixed Precision Policy æ·±åº¦å‰–æž](fsdp2_mixed_precision_policy_deep_dive.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šFSDP2 çš„æ··åˆç²¾åº¦æœºåˆ¶å¦‚ä½•å·¥ä½œï¼Ÿå‚æ•°ã€æ¢¯åº¦ã€Optimizer State å„ç”¨ä»€ä¹ˆç²¾åº¦ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- MixedPrecisionPolicy é…ç½®è¯¦è§£ï¼ˆactor.py:1042-1045ï¼‰
- å‚æ•°å­˜å‚¨ç²¾åº¦ï¼šSharded (FP32) vs Unsharded (BF16)
- æ¢¯åº¦ç´¯ç§¯ç²¾åº¦ï¼šå±€éƒ¨ (BF16) vs å½’çº¦ (FP32)
- Optimizer State ç²¾åº¦ç®¡ç†ï¼ˆFP32ï¼‰
- ä¸Ž autocast çš„æœ¬è´¨åŒºåˆ«

**é€‚åˆäººç¾¤**ï¼šéœ€è¦ç†è§£æ··åˆç²¾åº¦è®­ç»ƒæœºåˆ¶æˆ–è§£å†³ç²¾åº¦é—®é¢˜

**å…³é”®å‘çŽ°**ï¼š
- Sharded parameters å­˜å‚¨ä¸º FP32ï¼ˆä¿è¯é•¿æœŸç¨³å®šæ€§ï¼‰
- All-Gather æ—¶è½¬ä¸º BF16 ç”¨äºŽè®¡ç®—ï¼ˆèŠ‚çœæ˜¾å­˜å’Œè®¡ç®—æ—¶é—´ï¼‰
- æ¢¯åº¦å½’çº¦å¼ºåˆ¶ä½¿ç”¨ FP32ï¼ˆ`reduce_dtype`ï¼Œé¿å…æ•°å€¼é—®é¢˜ï¼‰
- Optimizer å…¨ç¨‹ FP32ï¼ˆæ— ç²¾åº¦è½¬æ¢å¼€é”€ï¼‰

**ç²¾åº¦æµç¨‹æ€»ç»“**ï¼š
```
Sharded Params (FP32)
  â†’ All-Gather â†’ Unsharded Params (BF16)
  â†’ Forward/Backward (BF16)
  â†’ Gradients (BF16)
  â†’ Reduce-Scatter (FP32)
  â†’ Optimizer Update (FP32)
```

**ä¸Ž Autocast å¯¹æ¯”**ï¼š
- Autocastï¼šæ¢¯åº¦å½’çº¦åœ¨ BF16ï¼ˆæ•°å€¼ä¸ç¨³å®šï¼‰
- FSDP2ï¼šæ¢¯åº¦å½’çº¦åœ¨ FP32ï¼ˆæ•°å€¼ç¨³å®šï¼ŒæŽ¨èï¼‰

---

### ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®å¤„ç†ä¸Žåºåˆ—ç®¡ç† (Problems 6-8)

#### 6. [Data Packingã€Attention Mask ä¸Ž Position IDs](fsdp2_data_packing_attention_and_positions.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šFSDP2 å¦‚ä½•å®žçŽ°æ•°æ®æ‰“åŒ…å¹¶å¤„ç†ä½ç½®ä¿¡æ¯ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- `pack_samples()` å®žçŽ°ï¼ˆdata_packing.py:48-135ï¼‰
- `cu_seqlens` ç”Ÿæˆä¸Ž Flash Attention é›†æˆ
- Position IDs è®¡ç®—é€»è¾‘ï¼ˆæ¯ä¸ª sample ç‹¬ç«‹ä»Ž 0 å¼€å§‹ï¼‰

**é€‚åˆäººç¾¤**ï¼šéœ€è¦å®žçŽ°é«˜æ•ˆæ•°æ®æ‰“åŒ…æˆ–ç†è§£ Flash Attention é›†æˆ

**å…³é”®å‘çŽ°**ï¼š
- slime å¼ºåˆ¶ä½¿ç”¨ varlen/thd æ•°æ®æ‰“åŒ…ï¼ˆæ—  paddingï¼‰
- `cu_seqlens` å®šä¹‰æ¯ä¸ª sample çš„è¾¹ç•Œï¼ˆç”¨äºŽ Flash Attentionï¼‰
- Position IDs åœ¨æ‰“åŒ…åŽé‡æ–°ç”Ÿæˆï¼ˆæ¯ä¸ª sample ç‹¬ç«‹ï¼‰

**æ€§èƒ½æå‡**ï¼š
- æ—  padding loss â†’ 33% æ•ˆçŽ‡æå‡ï¼ˆå¹³å‡ 25% æœ‰æ•ˆ tokenï¼‰
- Flash Attention â†’ å†…å­˜å ç”¨ä»Ž O(nÂ²) é™è‡³ O(n)

---

#### 6+. [Position Encodingã€cu_seqlens ä¸Žé«˜æ•ˆ Loss è®¡ç®—ï¼ˆæ·±åº¦å‰–æžï¼‰](fsdp2_position_encoding_cu_seqlens_and_loss_computation.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šPack åŽ Position Embedding å¦‚ä½•é‡ç½®ï¼Ÿcu_seqlens æ˜¯çº¯é€»è¾‘é•¿åº¦å—ï¼Ÿå¦‚ä½•é«˜æ•ˆè®¡ç®— Loss è€Œä¸ç”¨ loopï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- Position Encoding é‡ç½®æœºåˆ¶è¯¦è§£ï¼ˆdata_packing.py:74ï¼‰
- cu_seqlens çš„ç‰©ç†ç´¢å¼•è¯­ä¹‰ï¼ˆç´¯ç§¯åºåˆ—é•¿åº¦ï¼‰
- Flash Attention varlen æ¨¡å¼å·¥ä½œåŽŸç†
- é«˜æ•ˆ Loss è®¡ç®—ï¼štensor.split() vs Python loop
- Unpack æœºåˆ¶ä¸Žç´¢å¼•è¿˜åŽŸ
- æ€§èƒ½åˆ†æžä¸Žä¼˜åŒ–å»ºè®®

**é€‚åˆäººç¾¤**ï¼šéœ€è¦æ·±åº¦ç†è§£ data packing æœºåˆ¶æˆ–ä¼˜åŒ– loss è®¡ç®—æ€§èƒ½

**å…³é”®å‘çŽ°**ï¼š
- âœ… **Position IDs ç‹¬ç«‹é‡ç½®**ï¼šæ¯ä¸ª sequence ä»Ž 0 å¼€å§‹ï¼ˆ`list(range(len(tokens)))`ï¼‰
- âœ… **cu_seqlens æ˜¯ç‰©ç†ç´¢å¼•**ï¼š`[0, len1, len1+len2, ...]`ï¼Œæ ‡è®°è¾¹ç•Œ
- âœ… **cu_seqlens æ˜¯çº¯é€»è¾‘é•¿åº¦**ï¼šç”Ÿæˆæ—¶ä¸å« paddingï¼ˆä¼ ç»™ Flash Attention å‰å¯èƒ½ paddingï¼‰
- âœ… **é«˜æ•ˆ Loss è®¡ç®—**ï¼š`tensor.split(response_lengths)` O(1) æ“ä½œï¼Œè¿”å›ž views
- âš¡ **æ€§èƒ½æå‡**ï¼šsplit() æ¯” Python loop å¿« 10-50x

**å…·ä½“ç¤ºä¾‹**ï¼ˆ3 ä¸ª sequencesï¼‰ï¼š
```python
# æ‰“åŒ…å‰
Seq1: tokens=[101,102,...], position_ids=[0,1,...,511]
Seq2: tokens=[201,202,...], position_ids=[0,1,...,767]
Seq3: tokens=[301,302,...], position_ids=[0,1,...,255]

# æ‰“åŒ…åŽ
flat_tokens:      [101,102,..., 201,202,..., 301,302,...]
flat_position_ids:[0,1,...,511, 0,1,...,767, 0,1,...,255]  â† é‡ç½®ï¼
cu_seqlens:       [0,         512,        1280,      1536]
```

**Loss è®¡ç®—æ€§èƒ½**ï¼ˆ8 seqï¼Œavg 512 tokensï¼‰ï¼š
- Python loopï¼š~20 ms
- tensor.split()ï¼š~0.4 msï¼ˆ**50x å¿«**ï¼‰

---

#### 7. [åºåˆ—é•¿åº¦å‡è¡¡ä¸Ž OOM å¤„ç†](fsdp2_sequence_balancing_and_oom_handling.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šå¦‚ä½•é¿å…åºåˆ—é•¿åº¦ä¸å‡è¡¡å¯¼è‡´çš„ OOMï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- `balance_data_across_ranks()` å®žçŽ°ï¼ˆdata_packing.py:184-246ï¼‰
- `max_tokens_per_gpu` åŠ¨æ€æ‰¹é‡æŽ§åˆ¶
- OOM ä¿æŠ¤æœºåˆ¶ï¼ˆæœ€å¤§ token æ•°é™åˆ¶ï¼‰

**é€‚åˆäººç¾¤**ï¼šéœ€è¦å¤„ç†å¤§è§„æ¨¡å˜é•¿åºåˆ—è®­ç»ƒ

**å…³é”®å‘çŽ°**ï¼š
- æŒ‰æ€» token æ•°å‡è¡¡ï¼ˆè€Œéž sample æ•°ï¼‰
- å¯ç”¨ `--balance-data` + `--use-dynamic-batch-size`
- è‡ªåŠ¨æ‹†åˆ†è¿‡å¤§ batchï¼ˆé¿å…å•ä¸ª batch OOMï¼‰

**è´Ÿè½½å‡è¡¡æ•ˆæžœ**ï¼ˆ4 å¡ï¼Œæ€» 160K tokensï¼‰ï¼š
- ä¸å‡è¡¡ï¼šRank0=80K, Rank1=40K, Rank2=30K, Rank3=10Kï¼ˆRank0 OOMï¼‰
- å‡è¡¡åŽï¼šæ¯ä¸ª Rank â‰ˆ 40K tokensï¼ˆæ—  OOMï¼‰

---

#### 8. [CP Padding ä¸Ž Ring Flash Attention](fsdp2_cp_padding_and_ring_flash_attention.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šCP æ¨¡å¼ä¸‹å¦‚ä½•å¤„ç†åºåˆ—ä¸å¯¹é½é—®é¢˜ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- `pad_packed_sequence_with_cp()` å®žçŽ°ï¼ˆdata_packing.py:425-489ï¼‰
- å¡«å……ç­–ç•¥ï¼ˆå¡«å……åˆ° `cp_size` çš„å€æ•°ï¼‰
- Position IDs è¿žç»­æ€§ä¿æŒï¼ˆå¡«å……åŒºåŸŸä½¿ç”¨é€’å¢ž IDï¼‰

**é€‚åˆäººç¾¤**ï¼šéœ€è¦å®žçŽ° Context Parallelism çš„å¼€å‘è€…

**å…³é”®å‘çŽ°**ï¼š
- å¡«å……å‘ç”Ÿåœ¨ packed_sequence çš„åºåˆ—ç»´åº¦
- å¡«å……çš„ tokens ä¸å‚ä¸Ž loss è®¡ç®—ï¼ˆloss_mask=0ï¼‰
- Ring Flash Attention è¦æ±‚å„ rank è¾“å…¥é•¿åº¦ä¸€è‡´

**å¡«å……å¼€é”€**ï¼ˆcp_size=4ï¼Œæ€»é•¿åº¦=8193ï¼‰ï¼š
- å¡«å……å‰ï¼š8193 tokens
- å¡«å……åˆ°ï¼š8196 tokensï¼ˆ+3 tokensï¼Œ+0.04%ï¼‰

---

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šå†…å­˜ç®¡ç†ä¸Ž CPU Offload (Problems 9-11)

#### 9. [Sleep/Wake_up ä¸Ž CPU Offloading](fsdp2_sleep_wakeup_and_cpu_offloading.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šsleep/wake_up çš„å…·ä½“å®žçŽ°ä¸Žæ€§èƒ½å½±å“ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- `sleep()` å®žçŽ°ï¼ˆactor.py:276-288ï¼‰ï¼šå‚æ•° + ä¼˜åŒ–å™¨çŠ¶æ€ä¸€èµ· offload åˆ° CPU RAM
- `wake_up()` å®žçŽ°ï¼ˆactor.py:290-298ï¼‰ï¼šå®Œæ•´æ¢å¤åˆ° GPU
- `move_torch_optimizer()` è¾…åŠ©å‡½æ•°ï¼ˆactor.py:1181-1200ï¼‰

**é€‚åˆäººç¾¤**ï¼šéœ€è¦å®žçŽ°åŠ¨æ€å†…å­˜ç®¡ç†æˆ– CPU offload

**å…³é”®å‘çŽ°**ï¼š
- Offload ç›®æ ‡ï¼šCPU RAMï¼ˆéžç£ç›˜ï¼‰
- å¸¦å®½ç“¶é¢ˆï¼šPCIe 4.0 x16 â‰ˆ 25 GB/s
- é¦–æ¬¡ wake_up å¼€é”€ï¼š2-5 ç§’ï¼ˆ7B æ¨¡åž‹ï¼Œ14 GB å‚æ•°+çŠ¶æ€ï¼‰
- åŽç»­å¼€é”€ï¼šä»…åœ¨å‚æ•°æ›´æ–°æ—¶é‡æ–°ä¼ è¾“

---

#### 10. [Optimizer State ç”Ÿå‘½å‘¨æœŸç®¡ç†](fsdp2_optimizer_state_lifecycle.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šä¼˜åŒ–å™¨çŠ¶æ€åœ¨è®­ç»ƒåŽæ˜¯å¦é”€æ¯ï¼Ÿå¦‚ä½•ä¿æŒä¸€è‡´æ€§ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- Optimizer State ä¸ä¼šé”€æ¯ï¼ˆactor.py:447-465ï¼‰
- `sleep()` ä»…åœ¨åˆå§‹åŒ–è°ƒç”¨ä¸€æ¬¡ï¼ˆactor.py:233-242ï¼‰
- `wake_up()` å¹‚ç­‰æ€§ï¼ˆå¤šæ¬¡è°ƒç”¨æ— å½±å“ï¼‰
- State ä¸€è‡´æ€§é€šè¿‡ parameter-object æ˜ å°„ç»´æŠ¤

**é€‚åˆäººç¾¤**ï¼šéœ€è¦ç†è§£ä¼˜åŒ–å™¨çŠ¶æ€ç®¡ç†æœºåˆ¶

**å…³é”®å‘çŽ°**ï¼š
- âŒ **é”™è¯¯è®¤çŸ¥**ï¼šæ¯æ¬¡è®­ç»ƒåŽé”€æ¯ optimizer state
- âœ… **å®žé™…è¡Œä¸º**ï¼šState æŒä¹…åŒ–åœ¨ GPU/CPUï¼Œé€šè¿‡å‚æ•°å¯¹è±¡æ˜ å°„ç»´æŠ¤ä¸€è‡´æ€§
- âœ… **Offload æ—¶æœº**ï¼šä»…åœ¨ `offload_train=True` ä¸”é¦–æ¬¡è°ƒç”¨ `sleep()` æ—¶
- âœ… **Wake_up å¹‚ç­‰æ€§**ï¼šé‡å¤è°ƒç”¨ `wake_up()` ä¸ä¼šé‡å¤ä¼ è¾“æ•°æ®

**ç”Ÿå‘½å‘¨æœŸæµç¨‹**ï¼š
```
åˆå§‹åŒ– â†’ [sleep() ä¸€æ¬¡æ€§ offload] â†’ [è®­ç»ƒå¾ªçŽ¯å¼€å§‹]
  â†“
wake_up() â†’ train() â†’ (state ä¿æŒåœ¨ GPU) â†’ ä¸‹ä¸€è½® train()
  â†‘_______________________________________________|
```

**æ€§èƒ½æ•°æ®**ï¼ˆ7B æ¨¡åž‹ï¼ŒAdamWï¼ŒBF16ï¼‰ï¼š
- Optimizer State å¤§å°ï¼š14 GBï¼ˆ2x model paramsï¼‰
- é¦–æ¬¡ wake_up å¼€é”€ï¼š2-5 ç§’ï¼ˆPCIe 4.0ï¼‰
- åŽç»­è®­ç»ƒè¿­ä»£å¼€é”€ï¼š0 ç§’ï¼ˆstate å·²åœ¨ GPUï¼‰

---

#### 11. [Ref Model Offload ä¸Žå†…å­˜ç¢Žç‰‡åŒ–](fsdp2_ref_model_offload_and_memory_fragmentation.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šRef Model ä½¿ç”¨ FSDP2 åŽŸç”Ÿ offload è¿˜æ˜¯æ‰‹åŠ¨ to('cpu')ï¼Ÿç¢Žç‰‡åŒ–å·®å¼‚ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- Ref Model å§‹ç»ˆä½¿ç”¨ FSDP2 `CPUOffloadPolicy`ï¼ˆactor.py:768-809ï¼‰
- Actor Model æ··åˆç­–ç•¥ï¼ˆactor.py:307-377ï¼‰ï¼š
  - `fsdp_cpu_offload=True`ï¼šä¸¤æ¨¡åž‹éƒ½ç”¨ FSDP2 offloadï¼ˆå…±å­˜ GPUï¼‰
  - `fsdp_cpu_offload=False`ï¼šRef ç”¨ FSDP2 offloadï¼ŒActor æ‰‹åŠ¨ `model.cpu()`
- ç¢Žç‰‡åŒ–å¯¹æ¯”ï¼šFSDP2 offloadï¼ˆ1-5%ï¼‰vs æ‰‹åŠ¨ offloadï¼ˆ30-40%ï¼‰

**é€‚åˆäººç¾¤**ï¼šéœ€è¦å®žçŽ°å¤šæ¨¡åž‹å†…å­˜ç®¡ç†æˆ–ä¼˜åŒ–æ˜¾å­˜ç¢Žç‰‡

**å…³é”®å‘çŽ°**ï¼š
- **Ref Model å›ºå®šç­–ç•¥**ï¼šCPUOffloadPolicyï¼ˆæ— æ¡ä»¶ï¼‰
- **Actor Model åŠ¨æ€ç­–ç•¥**ï¼šåŸºäºŽ `fsdp_cpu_offload` æ ‡å¿—
- **ç¢Žç‰‡åŒ–æœºåˆ¶**ï¼š
  - FSDP2 offloadï¼šLayer-by-layer offload â†’ é¿å…å¤§å—ç©ºæ´ž
  - æ‰‹åŠ¨ offloadï¼šæ•´ä½“ offload â†’ äº§ç”Ÿç¢Žç‰‡åŒ–ç©ºæ´ž

**æ˜¾å­˜ç¢Žç‰‡åŒ–å¯¹æ¯”**ï¼ˆ7B æ¨¡åž‹ï¼Œå…± 14 GBï¼‰ï¼š

| æ–¹æ³• | ç¢Žç‰‡åŒ–çŽ‡ | å¯ç”¨è¿žç»­æ˜¾å­˜ | åŽŸå›  |
|------|---------|-------------|------|
| FSDP2 CPUOffloadPolicy | 1-5% | 13.3-13.9 GB | é€å±‚ offloadï¼Œç»†ç²’åº¦é‡Šæ”¾ |
| æ‰‹åŠ¨ model.cpu() | 30-40% | 8.4-9.8 GB | æ•´ä½“ offloadï¼Œäº§ç”Ÿç¢Žç‰‡ç©ºæ´ž |

**æ€§èƒ½å¼€é”€**ï¼ˆ7B æ¨¡åž‹ï¼Œfp32 offloadï¼‰ï¼š
- Offload æ—¶é—´ï¼šçº¦ 3-5 ç§’ï¼ˆFSDP2 ä¸Žæ‰‹åŠ¨ç›¸å½“ï¼‰
- Reload æ—¶é—´ï¼šçº¦ 2-4 ç§’ï¼ˆFSDP2 ç¨å¿«ï¼Œlayer-by-layer é¢„å–ï¼‰

---

#### 12. [CPU Offload å¼‚æ­¥ä¼ è¾“ä¸Žå†…å­˜ç®¡ç†](fsdp2_cpu_offload_async_transfer_and_memory_management.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šSleep/Wake_up æ˜¯å¦ä½¿ç”¨ pin_memory å¼‚æ­¥ä¼ è¾“ï¼Ÿæ˜¯å¦å­˜åœ¨å†…å­˜æ³„æ¼æˆ–ç¢Žç‰‡åŒ–é—®é¢˜ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- Optimizer states å¼‚æ­¥ä¼ è¾“åˆ†æžï¼ˆactor.py:1001-1013ï¼Œnon_blocking=Trueï¼‰
- Model parameters åŒæ­¥ä¼ è¾“é™åˆ¶ï¼ˆmodel.cpu()/cuda() ä¸æ”¯æŒ non_blockingï¼‰
- Pin_memory æœºåˆ¶è§£æžï¼ˆPyTorch è‡ªåŠ¨å¤„ç†ï¼Œä¸éœ€æ˜¾å¼è°ƒç”¨ï¼‰
- å†…å­˜ç®¡ç†ç­–ç•¥ï¼ˆgc.collectã€empty_cacheã€barrierï¼‰
- Python GC ä¸Ž PyTorch CUDA ç¼“å­˜äº¤äº’
- å†…å­˜æ³„æ¼é£Žé™©è¯„ä¼°ï¼ˆæ— æ˜Žç¡®è¯æ®ï¼‰
- æ˜¾å­˜ç¢Žç‰‡åŒ–åˆ†æžï¼ˆexpandable_segments é˜²æŠ¤ï¼‰

**é€‚åˆäººç¾¤**ï¼šéœ€è¦ç†è§£ CPU-GPU å†…å­˜ä¼ è¾“æœºåˆ¶æˆ–ä¼˜åŒ–å†…å­˜ç®¡ç†

**å…³é”®å‘çŽ°**ï¼š
- âœ… **Optimizer states ä½¿ç”¨å¼‚æ­¥ä¼ è¾“**ï¼š`value.to(device, non_blocking=True)`
- âŒ **Model parameters åŒæ­¥ä¼ è¾“**ï¼šPyTorch çš„ model.cpu()/cuda() ä¸æ”¯æŒ non_blocking
- âŒ **ä¸æ˜¾å¼ä½¿ç”¨ pin_memory()**ï¼šPyTorch å†…éƒ¨è‡ªåŠ¨å¤„ç†ï¼ˆæ˜¾å¼è°ƒç”¨åè€Œæ›´æ…¢ï¼‰
- âœ… **å®Œå–„çš„å†…å­˜æ¸…ç†æœºåˆ¶**ï¼šgc.collect() + torch.cuda.empty_cache()
- âœ… **ä½¿ç”¨ expandable_segments**ï¼šå‡å°‘ç¢Žç‰‡åŒ–ï¼ˆèŠ‚çœ 34% æ˜¾å­˜ï¼‰
- âŒ **æ— å†…å­˜æ³„æ¼è¯æ®**ï¼šä½†ç¼ºä¹ä¸»åŠ¨ç›‘æŽ§æœºåˆ¶

**å¼‚æ­¥ä¼ è¾“æ€§èƒ½**ï¼ˆ7B æ¨¡åž‹ï¼Œ14 GB optimizer statesï¼‰ï¼š
- åŒæ­¥æ¨¡å¼ï¼š3-4 ç§’
- å¼‚æ­¥æ¨¡å¼ï¼š1-2 ç§’ï¼ˆèŠ‚çœ 50%+ï¼‰

**å†…å­˜æ¸…ç†æµç¨‹**ï¼š
```
torch.cuda.synchronize() â†’ gc.collect() â†’ torch.cuda.empty_cache()
```

**ç¢Žç‰‡åŒ–é˜²æŠ¤**ï¼š
- PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
- å®šæœŸ empty_cache()
- åˆ†å¸ƒå¼ barrier åŒæ­¥

---

### ç¬¬å››éƒ¨åˆ†ï¼šContext Parallelism æ·±åº¦å‰–æž (Problems 13-15)

#### 13. [Embedding å±‚åˆ†ç‰‡ä¸Ž CP Input åˆ‡åˆ†](fsdp2_embedding_sharding_and_cp_input_splitting.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šCP æ¨¡å¼ä¸‹ input_ids è¢«åˆ‡åˆ†ï¼ŒEmbedding å±‚æ˜¯å¦ä¹Ÿåˆ‡åˆ†è®¡ç®—ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- **Embedding Table åˆ†ç‰‡ç»´åº¦**ï¼š`dp` ç»´åº¦ï¼ˆvocab_size ç»´åº¦åˆ‡åˆ†ï¼‰
- **CP ç»´åº¦çŠ¶æ€**ï¼šå‚æ•°åœ¨ cp ç»´åº¦å¤åˆ¶ï¼ˆå„ CP rank å­˜å‚¨ç›¸åŒ shardï¼‰
- **Input_ids åˆ‡åˆ†**ï¼šåºåˆ—ç»´åº¦åˆ‡åˆ†ï¼ˆactor.py:811-831ï¼‰
- **FSDP2 All-Gather**ï¼šForward å‰è‡ªåŠ¨ all-gather å®Œæ•´ vocab

**é€‚åˆäººç¾¤**ï¼šéœ€è¦ç†è§£ Embedding å±‚å¹¶è¡Œç­–ç•¥

**å…³é”®å‘çŽ°**ï¼š
- âŒ **é”™è¯¯è®¤çŸ¥**ï¼šEmbedding Table åœ¨ cp ç»´åº¦åˆ‡åˆ†
- âœ… **å®žé™…è¡Œä¸º**ï¼šEmbedding Table ä»…åœ¨ dp ç»´åº¦åˆ‡åˆ†ï¼ˆvocab ç»´åº¦ï¼‰
- âœ… **CP ç»´åº¦å¤åˆ¶**ï¼šå„ CP rank å­˜å‚¨ç›¸åŒçš„ vocab shard
- âœ… **è‡ªåŠ¨ All-Gather**ï¼šFSDP2 åœ¨ forward å‰ all-gatherï¼Œè¾“å‡ºåŽè‡ªåŠ¨é‡Šæ”¾

**é€šä¿¡å¼€é”€**ï¼ˆ7B æ¨¡åž‹ï¼Œvocab=151936ï¼Œdp=4ï¼Œcp=2ï¼‰ï¼š

| æ“ä½œ | é€šä¿¡é‡ï¼ˆæ¯ GPUï¼‰| é€šä¿¡ç»„ | é¢‘çŽ‡ |
|------|---------------|---------|------|
| Embedding All-Gather | 300 MB | dp_group (4 GPUs) | æ¯æ¬¡ forward |
| é‡Šæ”¾åˆ†ç‰‡ | 0 MB | - | Forward ç»“æŸåŽè‡ªåŠ¨ |

**å…³é”®ç»“è®º**ï¼šCP size å¢žåŠ **ä¸ä¼š**å¢žåŠ  Embedding å±‚çš„é€šä¿¡å¼€é”€ï¼ˆAll-Gather ä»…åœ¨ dp_group å†…è¿›è¡Œï¼‰

---

#### 14. [Ring Flash Attention ä¸Ž CP çŠ¶æ€ç»´æŒ](fsdp2_ring_flash_attention_and_cp_state_maintenance.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šRing Flash Attention åªä¼ é€’ KV å—ï¼ŸMLP å±‚æ˜¯å¦ç»´æŒ CP åˆ‡åˆ†çŠ¶æ€ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- **Ring Flash Attention é€šä¿¡æ¨¡å¼**ï¼šä»…ä¼ é€’ KVï¼ŒQ ä¸ä¼ é€’ï¼ˆzhuzilin/ring-flash-attentionï¼‰
- **Attention è¾“å‡ºçŠ¶æ€**ï¼šä¿æŒ CP-splitï¼ˆä¸åš all-gatherï¼‰
- **MLP å±‚çŠ¶æ€**ï¼šç»§ç»­åœ¨ CP-split çŠ¶æ€ä¸‹è®¡ç®—
- **å”¯ä¸€ All-Gather ç‚¹**ï¼šè®¡ç®— log_probs æ—¶ï¼ˆactor.py:888-977ï¼‰

**é€‚åˆäººç¾¤**ï¼šéœ€è¦ç†è§£ Ring Attention é€šä¿¡ä¼˜åŒ–å’Œ CP æ•°æ®æµ

**å…³é”®å‘çŽ°**ï¼š
- âœ… **Ring Flash Attention åªä¼  KV**ï¼šèŠ‚çœ 33% é€šä¿¡é‡ï¼ˆQ ä¸ä¼ é€’ï¼‰
- âœ… **æ•´ä¸ª Transformer Layer ä¿æŒ CP-split**ï¼šAttention â†’ MLP â†’ LayerNorm å…¨ç¨‹ CP-split
- âœ… **å”¯ä¸€ All-Gather ç‚¹**ï¼šlog_probs è®¡ç®—ï¼ˆéœ€è¦å®Œæ•´åºåˆ—ï¼‰

**é€šä¿¡å¼€é”€å¯¹æ¯”**ï¼ˆ7B æ¨¡åž‹ï¼Œseq_len=8192ï¼Œhidden=4096ï¼Œcp=4ï¼‰ï¼š

| æ“ä½œ | é€šä¿¡é‡ï¼ˆæ¯ GPUï¼‰| å¤‡æ³¨ |
|------|---------------|------|
| Ring Flash Attentionï¼ˆä»… KVï¼‰| 128 MB/layer | åœ¨ cp_group å†… ring ä¼ é€’ |
| å‡è®¾ä¼ é€’ QKV | 192 MB/layer | å¤š 33% é€šä¿¡é‡ |
| Log_probs All-Gather | 3 KB/sample | ä»…æœ€åŽä¸€æ­¥ï¼Œå¼€é”€å¯å¿½ç•¥ |

**CP çŠ¶æ€æµè½¬**ï¼š
```
Input (CP-split seq_len/4)
  â†’ Embedding (FSDP auto all-gather in dp_group)
  â†’ Attention (CP-split, Ring KV exchange in cp_group)
  â†’ MLP (CP-split)
  â†’ ... (æ‰€æœ‰ layers ä¿æŒ CP-split)
  â†’ Logits (CP-split)
  â†’ Log_probs è®¡ç®—ï¼ˆAll-Gather in cp_groupï¼Œå¾—åˆ°å®Œæ•´åºåˆ—ï¼‰
```

---

#### 15. [Monkey Patch æœºåˆ¶ä¸Žç‰ˆæœ¬å…¼å®¹æ€§](fsdp2_monkey_patch_mechanism_and_compatibility.md)
**æ ¸å¿ƒé—®é¢˜**ï¼šsubstitute_hf_flash_attn æ˜¯å¦ä½¿ç”¨ Monkey Patchï¼Ÿç‰ˆæœ¬å‡çº§å…¼å®¹æ€§å¦‚ä½•ï¼Ÿ

**å…³é”®å†…å®¹**ï¼š
- **Monkey Patch å®žçŽ°**ï¼šè¿è¡Œæ—¶æ›¿æ¢ `transformers.modeling_flash_attention_utils._flash_attention_forward`
- **å¤šç‰ˆæœ¬å…¼å®¹ç­–ç•¥**ï¼šç­¾ååŒ¹é…ï¼ˆv0-v3ï¼‰æ”¯æŒ Transformers 4.47.0 - 4.56.0+
- **å…¼å®¹æ€§é£Žé™©**ï¼šä¸­ç­‰ï¼ˆå‡½æ•°ç­¾åå˜åŒ–é£Žé™©é«˜ï¼Œå†…éƒ¨å®žçŽ°å˜åŒ–é£Žé™©ä¸­ç­‰ï¼‰
- **æ›¿ä»£æ–¹æ¡ˆå¯¹æ¯”**ï¼šç»§æ‰¿é‡å†™ã€è‡ªå®šä¹‰æ¨¡åž‹ã€ç­‰å¾… PyTorch åŽŸç”Ÿæ”¯æŒ

**é€‚åˆäººç¾¤**ï¼šéœ€è¦ç†è§£ç”Ÿæ€å…¼å®¹æ€§å®žçŽ°æˆ–è¯„ä¼° Monkey Patch é£Žé™©

**å…³é”®å‘çŽ°**ï¼š
- âœ… **ç¡®è®¤ Monkey Patch**ï¼šç›´æŽ¥æ›¿æ¢ HuggingFace å†…éƒ¨å‡½æ•°
- âœ… **å¤šç‰ˆæœ¬ç­¾ååŒ¹é…**ï¼š`create_ring_flash_attention_forward` ç”Ÿæˆ v0-v3 ç­¾å
- âš ï¸ **å…¼å®¹æ€§é£Žé™©**ï¼šTransformers å‡½æ•°ç­¾åå˜åŒ–æ—¶éœ€è¦æ›´æ–°
- âœ… **é™çº§è·¯å¾„**ï¼š`RING_ATTN_SWITCH` æ ‡å¿—å¯åŠ¨æ€åˆ‡æ¢å›žåŽŸå§‹å®žçŽ°

**ç‰ˆæœ¬å…¼å®¹æ€§çŸ©é˜µ**ï¼š

| Transformers ç‰ˆæœ¬ | ç­¾åç‰ˆæœ¬ | å…¼å®¹æ€§ | è¯´æ˜Ž |
|------------------|---------|--------|------|
| 4.47.0 - 4.50.x | v0 | âœ… å®Œå…¨å…¼å®¹ | åˆå§‹ç­¾å |
| 4.51.0 - 4.53.x | v1 | âœ… å®Œå…¨å…¼å®¹ | æ–°å¢žå‚æ•° |
| 4.54.0 - 4.56.x | v2 | âœ… å®Œå…¨å…¼å®¹ | è¿›ä¸€æ­¥æ‰©å±• |
| 4.57.0+ | v3 | âœ… é¢„æµ‹å…¼å®¹ | æœªæ¥ç‰ˆæœ¬ |
| 5.0.0+ | ??? | âš ï¸ éœ€æ›´æ–° | Major ç‰ˆæœ¬å¯èƒ½å¤§æ”¹ |

**æœ€ä½³å®žè·µ**ï¼š
1. **ç‰ˆæœ¬é”å®š**ï¼š`transformers>=4.47.0,<5.0.0`
2. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šå¯åŠ¨æ—¶éªŒè¯ `check_params()` åŒ¹é…æˆåŠŸ
3. **é™çº§è·¯å¾„**ï¼šä¿ç•™ `RING_ATTN_SWITCH` å¼€å…³
4. **æŒç»­ç›‘æŽ§**ï¼šè·Ÿè¸ª Transformers å‘å¸ƒè¯´æ˜Ž

---

## ðŸŽ¯ å­¦ä¹ è·¯å¾„å»ºè®®

### åˆå­¦è€…è·¯å¾„ï¼ˆé¦–æ¬¡æŽ¥è§¦ FSDP2ï¼‰
1. é˜…è¯»æ–‡æ¡£ 1ï¼šFSDP2 åŸºç¡€å®žçŽ°
2. é˜…è¯»æ–‡æ¡£ 2ï¼šDeviceMesh ä¸Žåˆ†ç‰‡æœºåˆ¶
3. é˜…è¯»æ–‡æ¡£ 5ï¼šMixed Precision Policyï¼ˆç†è§£ç²¾åº¦ç®¡ç†ï¼‰
4. é˜…è¯»æ–‡æ¡£ 6ï¼šData Packing ä¸Žæ•°æ®æµ
5. é˜…è¯»æ–‡æ¡£ 9ï¼šSleep/Wake_up åŸºç¡€

### è¿›é˜¶è·¯å¾„ï¼ˆéœ€è¦å®žçŽ° FSDP2ï¼‰
1. æ–‡æ¡£ 1-5ï¼šæ ¸å¿ƒæž¶æž„å…¨é¢ç†è§£ï¼ˆå«æ··åˆç²¾åº¦ï¼‰
2. æ–‡æ¡£ 6-8ï¼šæ•°æ®å¤„ç†å®Œæ•´æµç¨‹
3. æ–‡æ¡£ 9-12ï¼šå†…å­˜ç®¡ç†ç­–ç•¥ï¼ˆå«å¼‚æ­¥ä¼ è¾“ï¼‰
4. æ–‡æ¡£ 3ï¼šCheckpoint ç³»ç»Ÿå®žçŽ°

### ä¸“å®¶è·¯å¾„ï¼ˆä¼˜åŒ–æˆ–å¤åˆ¶ FSDP2ï¼‰
1. å®Œæ•´é˜…è¯»æ‰€æœ‰ 15 ç¯‡æ–‡æ¡£
2. é‡ç‚¹å…³æ³¨æ–‡æ¡£ 13-15ï¼šCP æ·±åº¦ä¼˜åŒ–
3. ç ”ç©¶é€šä¿¡å¼€é”€è®¡ç®—å…¬å¼
4. åˆ†æžç¢Žç‰‡åŒ–ä¼˜åŒ–ç­–ç•¥

### é—®é¢˜å¯¼å‘è·¯å¾„
- **æ˜¾å­˜ä¼˜åŒ–**ï¼šæ–‡æ¡£ 9, 10, 11, 12ï¼ˆOffload ä¸Žç¢Žç‰‡åŒ–ã€å¼‚æ­¥ä¼ è¾“ï¼‰
- **é€šä¿¡ä¼˜åŒ–**ï¼šæ–‡æ¡£ 2, 13, 14ï¼ˆDeviceMesh ä¸Ž Ring Attentionï¼‰
- **æ•°æ®æ•ˆçŽ‡**ï¼šæ–‡æ¡£ 6, 7, 8ï¼ˆData Packing ä¸Žåºåˆ—å‡è¡¡ï¼‰
- **ç²¾åº¦é—®é¢˜**ï¼šæ–‡æ¡£ 5ï¼ˆMixed Precision Policyï¼‰
- **å†…å­˜ç®¡ç†**ï¼šæ–‡æ¡£ 12ï¼ˆå¼‚æ­¥ä¼ è¾“ã€GCã€ç¢Žç‰‡åŒ–ï¼‰
- **ç”Ÿæ€é›†æˆ**ï¼šæ–‡æ¡£ 3, 15ï¼ˆCheckpoint ä¸Ž Monkey Patchï¼‰

---

## ðŸ“Š å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

### æ˜¾å­˜å ç”¨ï¼ˆ7B æ¨¡åž‹ï¼ŒBF16ï¼‰
| ç»„ä»¶ | å¤§å° | å¤‡æ³¨ |
|------|------|------|
| æ¨¡åž‹å‚æ•° | 7 GB | FSDP åˆ†ç‰‡åŽæ¯å¡ï¼š7GB / dp_size |
| æ¿€æ´»å€¼ | å˜åŠ¨ | å–å†³äºŽ batch size å’Œ seq_len |
| Optimizer State | 14 GB | AdamWï¼ˆ2x paramsï¼‰ï¼ŒFSDP åˆ†ç‰‡åŽï¼š14GB / dp_size |
| æ¢¯åº¦ | 7 GB | FSDP åˆ†ç‰‡åŽï¼š7GB / dp_size |

### é€šä¿¡å¼€é”€ï¼ˆ7B æ¨¡åž‹ï¼Œseq_len=4096ï¼Œdp=4ï¼Œcp=2ï¼‰
| æ“ä½œ | é€šä¿¡é‡ | é€šä¿¡ç»„ | é¢‘çŽ‡ |
|------|--------|--------|------|
| FSDP æ¢¯åº¦ All-Reduce | 25.2 GB/step | dp_group | æ¯ä¸ªè®­ç»ƒæ­¥ |
| Ring Attention KV ä¼ é€’ | 128 MB/layer | cp_group | æ¯ä¸ª forwardï¼ˆ32 layersï¼‰|
| Embedding All-Gather | 300 MB | dp_group | æ¯æ¬¡ forward |
| Log_probs All-Gather | 3 KB | cp_group | æ¯æ¬¡ forward |

### CPU Offload æ€§èƒ½ï¼ˆ7B æ¨¡åž‹ï¼ŒPCIe 4.0ï¼‰
| æ“ä½œ | æ—¶é—´ | æ•°æ®é‡ | å¸¦å®½åˆ©ç”¨çŽ‡ |
|------|------|--------|-----------|
| Sleep (é¦–æ¬¡) | 4-6 ç§’ | 21 GB (params+state) | 75% |
| Wake_up (é¦–æ¬¡) | 2-5 ç§’ | 21 GB | 80% |
| åŽç»­ Wake_up | ~0 ç§’ | 0 GB | - |

---

## ðŸ” æ ¸å¿ƒå®žçŽ°æ–‡ä»¶ç´¢å¼•

### ä¸»è¦æºç æ–‡ä»¶
- **`slime/backends/fsdp_utils/actor.py`**ï¼šFSDP2 æ ¸å¿ƒå®žçŽ°ï¼ˆ1263 è¡Œï¼‰
  - apply_fsdp2: 1016-1057
  - DeviceMesh åˆå§‹åŒ–: 165-209
  - sleep/wake_up: 276-298
  - Ref Model åˆ›å»º: 768-809
  - CP input åˆ‡åˆ†: 811-831
  - Log_probs All-Gather: 888-977

- **`slime/backends/fsdp_utils/data_packing.py`**ï¼šæ•°æ®æ‰“åŒ…ä¸Žåºåˆ—å¤„ç†
  - pack_samples: 48-135
  - balance_data_across_ranks: 184-246
  - pad_packed_sequence_with_cp: 425-489

- **`slime/backends/fsdp_utils/checkpoint.py`**ï¼šåˆ†å¸ƒå¼ Checkpoint
  - save_checkpoint: 93-146
  - OptimizerState: 32-46

- **External: ring-flash-attention**ï¼ˆGitHub: zhuzilin/ring-flash-attentionï¼‰
  - substitute_hf_flash_attn: hf_adapter.py
  - create_ring_flash_attention_forward: å¤šç‰ˆæœ¬ç­¾åç”Ÿæˆ

---

## ðŸ’¡ å…³é”®è®¾è®¡å†³ç­–æ€»ç»“

### 1. ä¸ºä»€ä¹ˆ FSDP2 ä¸æ”¯æŒ torch.compileï¼Ÿ
- DTensor ä¾èµ–åŠ¨æ€è®¡ç®—å›¾ï¼ˆç¼–è¯‘å™¨æ— æ³•é™æ€åˆ†æžï¼‰
- Data Packing å¯¼è‡´æ¯ä¸ª batch çš„ `cu_seqlens` ä¸åŒï¼ˆåŠ¨æ€å½¢çŠ¶ï¼‰
- Trade-offï¼šçµæ´»æ€§ï¼ˆå˜é•¿åºåˆ—ã€åŠ¨æ€æ‰¹é‡ï¼‰vs ç¼–è¯‘ä¼˜åŒ–

### 2. ä¸ºä»€ä¹ˆ Ref Model å›ºå®šä½¿ç”¨ CPUOffloadPolicyï¼Ÿ
- å‡å°‘æ˜¾å­˜ç¢Žç‰‡åŒ–ï¼ˆ1-5% vs 30-40%ï¼‰
- æ€§èƒ½ç›¸å½“æˆ–æ›´å¥½ï¼ˆlayer-by-layer é¢„å–ï¼‰
- ç®€åŒ–å®žçŽ°ï¼ˆFSDP2 è‡ªåŠ¨ç®¡ç† offload/reloadï¼‰

### 3. ä¸ºä»€ä¹ˆ Embedding Table åœ¨ dp ç»´åº¦åˆ‡åˆ†è€Œéž cpï¼Ÿ
- CP æ˜¯åºåˆ—å¹¶è¡Œï¼ˆsequence parallelismï¼‰ï¼Œä¸æ˜¯å¼ é‡å¹¶è¡Œ
- Embedding lookup éœ€è¦å®Œæ•´ vocabï¼ˆå¦åˆ™éœ€è¦ all-to-allï¼‰
- DP åˆ‡åˆ† + All-Gather æ›´é«˜æ•ˆï¼ˆé€šä¿¡é‡æ›´å°ï¼‰

### 4. ä¸ºä»€ä¹ˆ Ring Flash Attention åªä¼  KVï¼Ÿ
- Q ç”¨äºŽè®¡ç®—å½“å‰ rank çš„ Attentionï¼ˆä¸éœ€è¦ä¼ é€’ï¼‰
- åªéœ€è¦å…¶ä»– rank çš„ KV æ¥è®¡ç®—å®Œæ•´ Attention
- èŠ‚çœ 33% é€šä¿¡é‡ï¼ˆ2/3 vs 3/3ï¼‰

### 5. ä¸ºä»€ä¹ˆä½¿ç”¨ Monkey Patch è€Œéžç»§æ‰¿ï¼Ÿ
- HuggingFace æ¨¡åž‹ä¸æ”¯æŒè‡ªå®šä¹‰ Attention æ›¿æ¢
- Monkey Patch æ— éœ€ä¿®æ”¹ HuggingFace æºç 
- é£Žé™©å¯æŽ§ï¼ˆå¤šç‰ˆæœ¬ç­¾ååŒ¹é… + é™çº§è·¯å¾„ï¼‰

---

## ðŸš€ å®žçŽ° FSDP2 çš„æœ€å°å¿…éœ€æ­¥éª¤

å¦‚æžœè¦åœ¨å…¶ä»–æ¡†æž¶ä¸­å¤åˆ¶ FSDP2ï¼Œä»¥ä¸‹æ˜¯å…³é”®æ­¥éª¤ï¼š

### 1. åŸºç¡€å¹¶è¡Œ
1. å®žçŽ° 2D DeviceMeshï¼ˆDP + CPï¼‰
2. å®žçŽ° Layer-wise å‚æ•°åˆ†ç‰‡ï¼ˆDTensor æˆ–ç­‰ä»·æœºåˆ¶ï¼‰
3. å®žçŽ° All-Gatherï¼ˆforwardï¼‰+ æ¢¯åº¦ Reduce-Scatterï¼ˆbackwardï¼‰

### 2. æ•°æ®å¤„ç†
1. å®žçŽ° varlen Data Packingï¼ˆç§»é™¤ paddingï¼‰
2. å®žçŽ° `cu_seqlens` ç”Ÿæˆï¼ˆFlash Attention é›†æˆï¼‰
3. å®žçŽ°æŒ‰ token æ•°å‡è¡¡ï¼ˆbalance_dataï¼‰

### 3. å†…å­˜ä¼˜åŒ–
1. å®žçŽ° CPUOffloadPolicyï¼ˆlayer-by-layer offloadï¼‰
2. å®žçŽ° sleep/wake_up ç”Ÿå‘½å‘¨æœŸç®¡ç†
3. å®žçŽ° Optimizer State æŒä¹…åŒ–

### 4. Context Parallelism
1. å®žçŽ° Ring Flash Attentionï¼ˆä»…ä¼  KVï¼‰
2. å®žçŽ° CP çŠ¶æ€ç»´æŒï¼ˆæ•´ä¸ª Transformer Layerï¼‰
3. å®žçŽ° Log_probs All-Gather

### 5. ç”Ÿæ€é›†æˆ
1. å®žçŽ°åˆ†å¸ƒå¼ Checkpointï¼ˆtorch.distributed.checkpointï¼‰
2. å®žçŽ° HuggingFace æ ¼å¼è½¬æ¢
3. å®žçŽ° Monkey Patch æˆ–æ›¿ä»£æœºåˆ¶

---

## ðŸ“® åé¦ˆä¸Žæ›´æ–°

æœ¬æ–‡æ¡£é›†ç”± Claude Code åŸºäºŽ slime æ¡†æž¶æºç åˆ†æžç”Ÿæˆã€‚

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0
**åŸºäºŽä»£ç ç‰ˆæœ¬**ï¼šslime main branch (commit: 9d7f34d)
**ç”Ÿæˆæ—¥æœŸ**ï¼š2025-12-04

**è”ç³»æ–¹å¼**ï¼š
- å¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨ slime GitHub ä»“åº“æ Issue
- å¦‚å‘çŽ°æ–‡æ¡£é”™è¯¯ï¼Œè¯·æäº¤ PR ä¿®æ­£

---

## ðŸŽ“ è‡´è°¢

æ„Ÿè°¢ slime å›¢é˜Ÿå¼€æºé«˜è´¨é‡çš„ FSDP2 å®žçŽ°ä»£ç ï¼Œä¸ºç¤¾åŒºæä¾›äº†å®è´µçš„å­¦ä¹ èµ„æºã€‚

ç‰¹åˆ«æ„Ÿè°¢ï¼š
- PyTorch å›¢é˜Ÿï¼ˆFSDP2 æ ¸å¿ƒå®žçŽ°ï¼‰
- HuggingFace å›¢é˜Ÿï¼ˆTransformers ç”Ÿæ€ï¼‰
- ring-flash-attention é¡¹ç›®ï¼ˆRing Attention å®žçŽ°ï¼‰

---

**Happy Learning!** ðŸš€
